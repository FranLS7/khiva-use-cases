{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khiva's features extraction application \n",
    "This interactive application shows some of the capabilities provided by the Khiva library for times-series’ feature extraction and machine learning. \n",
    "This use case consists on the analysis of 100 time-series provided by commercial sites during 2012 which are tagged by subindustry.  \n",
    "This exercise is focused on: \n",
    "1. Extract the time series features. \n",
    "2. Predict the subindustry of some sites. \n",
    "\n",
    "## Module importing \n",
    "We are using Khiva’s array, features and library modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khiva.features import *\n",
    "from khiva.library import *\n",
    "from khiva.array import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata load \n",
    "This is anonymised 5-minute energy usage data for 100 commercial/ industrial sites for 2012. \n",
    "The sites metadata contains the site ID, the industry, square footage, lat/lng and timezone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = pd.read_csv(\"../../energy/data/data-enerNoc/all-data/meta/all_sites.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the site id for each site \n",
    "This is needed to check the accuracy of the predictive modelling step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "\n",
    "for name in all_sites[\"SITE_ID\"].values:\n",
    "    file_names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend\n",
    "Prints the backend used. CPU, CUDA and OPENCL backends are available for Khiva.  \n",
    "  \n",
    "> This interactive application is being execute in **hub.mybinder** which doesn't provide a GPU and its CPU is quite limited so the features extraction is going to take some time. This application executed in a macOS High Sierra with a 2,9 GHz Intel Core i7 processor takes 8,34 seconds in extracting the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KHIVABackend.KHIVA_BACKEND_CPU\n"
     ]
    }
   ],
   "source": [
    "print(get_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load\n",
    "The original dataset contains 100 time-series, which was a total of 10,531,288 data points. This original dataset was re-dimensioned using Khiva and the result is a dataset of 100 time-series which is a total of 1,666,600 data points. After the re-dimension, the dataset was stored in a binary and then loaded into a Khiva Array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_tmp  = Array(np.load(\"../../energy/electric-consumption-rate-python/time-series-redimension-applied.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction \n",
    "In this step, we use Khiva to extract 28 features from the time-series, so we can generate a features matrix for applying a predictive modelling. As explained before, this interactive application is being executed in **hub.mybinder**, without GPUs and with a limited CPU, so it is going to take a while as the time series are compound by a big amount of data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract the features : 8.75836968421936 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "features = np.stack([abs_energy(arr_tmp).to_numpy(),\n",
    "                    absolute_sum_of_changes(arr_tmp).to_numpy(),\n",
    "                    count_above_mean(arr_tmp).to_numpy(),\n",
    "                    count_below_mean(arr_tmp).to_numpy(),\n",
    "                    first_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                    first_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                    has_duplicates(arr_tmp).to_numpy(),\n",
    "                    has_duplicate_max(arr_tmp).to_numpy(),\n",
    "                    kurtosis(arr_tmp).to_numpy(),\n",
    "                    last_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                    last_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                    has_duplicate_min(arr_tmp).to_numpy(),\n",
    "                    longest_strike_above_mean(arr_tmp).to_numpy(),\n",
    "                    longest_strike_below_mean(arr_tmp).to_numpy(),\n",
    "                    maximum(arr_tmp).to_numpy(),\n",
    "                    mean_absolute_change(arr_tmp).to_numpy(),\n",
    "                    minimum(arr_tmp).to_numpy(),\n",
    "                    number_crossing_m(arr_tmp, 0).to_numpy(),\n",
    "                    mean(arr_tmp).to_numpy(),\n",
    "                    median(arr_tmp).to_numpy(),\n",
    "                    mean_change(arr_tmp).to_numpy(),\n",
    "                    ratio_value_number_to_time_series_length(arr_tmp).to_numpy(),\n",
    "                    skewness(arr_tmp).to_numpy(),\n",
    "                    standard_deviation(arr_tmp).to_numpy(),\n",
    "                    sum_of_reoccurring_values(arr_tmp).to_numpy(),\n",
    "                    sum_values(arr_tmp).to_numpy(),\n",
    "                    variance(arr_tmp).to_numpy(),\n",
    "                    variance_larger_than_standard_deviation(arr_tmp).to_numpy()\n",
    "                            ])\n",
    "print(\"Time to extract the features : \" + str(time.time() - start) + \" seconds.\" )\n",
    "features = features.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features matrix and target definition.\n",
    "Here, the features matrix and target matrix are defined in the following way: \n",
    "1. **Features matrix** Composed by the 28 features extracted. \n",
    "2. **Target matrix** Composed by the subindustries’ tag of each time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_sites[\"SUB_INDUSTRY\"].values\n",
    "X = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features matrix pre-process \n",
    "A simple pre-process is executed to scale the features matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle \n",
    "Several shuffles are done to distribute the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    X, y, file_names = shuffle(X, y, file_names, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive modelling \n",
    "In this step, we create a model, fit it and predict a subset of samples. \n",
    "\n",
    "The reason to choose SVC as classifier are the following: \n",
    "* More than 50 samples for the training and less than 100k in total. \n",
    "* The intention to predict a category.  \n",
    "* All data is labelled. \n",
    "\n",
    "We can conclude that the results shown are quite decent after using this classificator. \n",
    "\n",
    "The parameters chosen are based on a grid search based on a cross-validation(CV) step, focused on the accuracy of the model and using a K-FOLD(K=10) CV method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-52-eda90f6eb367>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-eda90f6eb367>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    print(\"PREDICTION VECTOR## Predictive modelling\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "files_test = []\n",
    "list_test_indices = []\n",
    "for i in range(len(file_names)):\n",
    "    if file_names[i] in [92, 45, 761, 10, 766, 400, 673, 49, 144, 496, 731, 281, 213, 197, 399]:\n",
    "        list_test_indices.append(i)\n",
    "        files_test.append(file_names[i])\n",
    "\n",
    "X_train = np.delete(X, list_test_indices, 0)\n",
    "X_test = np.take(X, list_test_indices, 0)\n",
    "y_train = np.delete(y, list_test_indices)\n",
    "y_test = np.take(y, list_test_indices)\n",
    "\n",
    "k_range_parameter = {'degree':[3,4],'shrinking':[True,False],'probability':[True,False]}\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "mygridsearch = GridSearchCV(clf, k_range_parameter, cv = 10, scoring = 'accuracy' )\n",
    "mygridsearch.fit(X_train, y_train)\n",
    "bestclassifier = mygridsearch.best_estimator_\n",
    "y_pred = bestclassifier.predict(X_test)\n",
    "print(\"TEST VECTOR: \" + str(y_test))\n",
    "print(\"PREDICTION VECTOR\" + str(y_pred))\n",
    "print(\"NUMBER OF ERRORS: \" + str(sum(y_pred != y_test)))\n",
    "print(\"ERROR RATE: \" + str(1 - sum(y_pred == y_test) / float(len(y_pred))) + \"%\")\n",
    "print(\"ACCURACY: \" + str(sum(y_pred == y_test) / float(len(y_pred))) + \"%\")\n",
    "print(\"PARAMETERS USED: \"+ str(mygridsearch.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
