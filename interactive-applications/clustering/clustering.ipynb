{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Time series clustering using Khiva\n",
    "\n",
    "Clustering time series is a very important method for the analysis of time series since it allows grouping the time series according to its characteristics to proceed to the correct or desired analysis / prediction on them.\n",
    "\n",
    "Thanks to the ability to extract characteristics from [Khiva](http://khiva-python.readthedocs.io/en/latest/), we can generate a [feature](http://khiva-python.readthedocs.io/en/latest/khiva.html#module-khiva.features) matrix to apply any of the clustering methods available and gets our desired clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from khiva.features import *\n",
    "from khiva.array import *\n",
    "from khiva.library import * \n",
    "from khiva.dimensionality import *\n",
    "\n",
    "import pandas as df\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 5]\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we want to clusterize the time series?\n",
    "\n",
    "Actually, the clustering of time series can have a wide variety of uses and applications depending on the context in which it is used.\n",
    "\n",
    "As an example, time series clustering can be used in order to classify them with the target of knowing what time series have got the same characteristics and that way, know which of them could be suitable to concrete forecasting methods (Moving average, exponential smoothing, box-jenkins, x-11, etc), as an example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend\n",
    "Prints the backend used. CPU, CUDA and OPENCL backends are available for Khiva.  \n",
    "  \n",
    "> This interactive application is being execute in **hub.mybinder** which doesn't provide a GPU and its CPU is quite limited so the features extraction is going to take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KHIVABackend.KHIVA_BACKEND_CPU\n"
     ]
    }
   ],
   "source": [
    "print(get_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction\n",
    "\n",
    "Next, a total number of 53 features are going to be extracted from 100 time series after reducing the dimensions of those ones to 1000 points in order to facilitate the computation. \n",
    "\n",
    "As this example is being executed in **hub.mybinder**, we don't want to execute the computation as it is going to be very time-consuming, so the next cell is already executed and the result (The feautres matrix) is already saved in binary format in: `./features_extracted.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../../energy/data/data-enerNoc/all-data/csv'\n",
    "#\n",
    "#file_names = []\n",
    "#features_list = []\n",
    "#i = 0\n",
    "#\n",
    "#for filename in os.listdir(path):\n",
    "#    if \".csv\" in filename:\n",
    "#        print(filename)\n",
    "#\n",
    "#        file_names.append(filename)\n",
    "#        data = pd.read_csv(path + \"/\" + filename)\n",
    "#        a = visvalingam(Array([range(len(data[\"value\"])), data[\"value\"].as_matrix()]), int(1000))\n",
    "#        arr_tmp = a.get_col(1)\n",
    "#\n",
    "#        features = np.stack([abs_energy(arr_tmp).to_numpy(),\n",
    "#                             absolute_sum_of_changes(arr_tmp).to_numpy(),\n",
    "#                             aggregated_autocorrelation(arr_tmp, 0).to_numpy(),\n",
    "#                             aggregated_autocorrelation(arr_tmp, 1).to_numpy(),\n",
    "#                             aggregated_autocorrelation(arr_tmp, 2).to_numpy(),\n",
    "#                             aggregated_autocorrelation(arr_tmp, 3).to_numpy(),\n",
    "#                             aggregated_autocorrelation(arr_tmp, 4).to_numpy(),\n",
    "#                             aggregated_autocorrelation(arr_tmp, 5).to_numpy(),\n",
    "#                             approximate_entropy(arr_tmp, 4, 0.5).to_numpy(),\n",
    "#                             binned_entropy(arr_tmp, 5).to_numpy(),\n",
    "#                             c3(arr_tmp, True).to_numpy(),\n",
    "#                             count_above_mean(arr_tmp).to_numpy(),\n",
    "#                             count_below_mean(arr_tmp).to_numpy(),\n",
    "#                             cwt_coefficients(arr_tmp, Array([1, 2, 3], dtype.s32), 2, 2).to_numpy(),\n",
    "#                             energy_ratio_by_chunks(arr_tmp, 2, 0).to_numpy(),\n",
    "#                             first_location_of_maximum(arr_tmp).to_numpy(),\n",
    "#                             first_location_of_minimum(arr_tmp).to_numpy(),\n",
    "#                             has_duplicates(arr_tmp).to_numpy(),\n",
    "#                             has_duplicate_max(arr_tmp).to_numpy(),\n",
    "#                             has_duplicate_min(arr_tmp).to_numpy(),\n",
    "#                             index_mass_quantile(arr_tmp, 0.5).to_numpy(),\n",
    "#                             kurtosis(arr_tmp).to_numpy(),\n",
    "#                             large_standard_deviation(arr_tmp, 0.4).to_numpy(),\n",
    "#                             last_location_of_maximum(arr_tmp).to_numpy(),\n",
    "#                             last_location_of_minimum(arr_tmp).to_numpy(),\n",
    "#                             length(arr_tmp).to_numpy(),\n",
    "#                             longest_strike_above_mean(arr_tmp).to_numpy(),\n",
    "#                             longest_strike_below_mean(arr_tmp).to_numpy(),\n",
    "#                             max_langevin_fixed_point(arr_tmp, 7, 2).to_numpy(),\n",
    "#                             maximum(arr_tmp).to_numpy(),\n",
    "#                             mean(arr_tmp).to_numpy(),\n",
    "#                             mean_absolute_change(arr_tmp).to_numpy(),\n",
    "#                             mean_change(arr_tmp).to_numpy(),\n",
    "#                             mean_second_derivative_central(arr_tmp).to_numpy(),\n",
    "#                             median(arr_tmp).to_numpy(),\n",
    "#                             minimum(arr_tmp).to_numpy(),\n",
    "#                             number_crossing_m(arr_tmp, 0).to_numpy(),\n",
    "#                             number_cwt_peaks(arr_tmp, 2).to_numpy(),\n",
    "#                             number_peaks(arr_tmp, 2).to_numpy(),\n",
    "#                             percentage_of_reoccurring_datapoints_to_all_datapoints(arr_tmp, False).to_numpy(),\n",
    "#                             percentage_of_reoccurring_values_to_all_values(arr_tmp, False).to_numpy(),\n",
    "#                             quantile(arr_tmp, Array([0.6], dtype.f32)).to_numpy(),\n",
    "#                             ratio_beyond_r_sigma(arr_tmp, 0.5).to_numpy(),\n",
    "#                             ratio_value_number_to_time_series_length(arr_tmp).to_numpy(),\n",
    "#                             skewness(arr_tmp).to_numpy(),\n",
    "#                             standard_deviation(arr_tmp).to_numpy(),\n",
    "#                             sum_of_reoccurring_values(arr_tmp).to_numpy(),\n",
    "#                             sum_values(arr_tmp).to_numpy(),\n",
    "#                             symmetry_looking(arr_tmp, 0.1).to_numpy(),\n",
    "#                             time_reversal_asymmetry_statistic(arr_tmp, 2).to_numpy(),\n",
    "#                             value_count(arr_tmp, mean(arr_tmp).to_numpy()).to_numpy(),\n",
    "#                             variance(arr_tmp).to_numpy(),\n",
    "#                             int(variance_larger_than_standard_deviation(arr_tmp).to_numpy())])\n",
    "#\n",
    "#        features_list.append(features)\n",
    "#np.save(\"features_extracted\", np.array(features_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Next, the 53 features used in order to clusterize the time sereis are shown. As it has been explained before, the features extracted are loaded from a binary file located in `./features_extracted.npy`.\n",
    "\n",
    "> Note: Remember to visit the [Khiva's documentation](http://khiva-python.readthedocs.io/en/latest/khiva.html#module-khiva.features) in order to understand the features extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_energy(arr)</th>\n",
       "      <th>absolute_sum_of_changes(arr)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 0)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 1)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 2)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 3)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 4)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 5)</th>\n",
       "      <th>approximate_entropy(arr, 4, 0.5)</th>\n",
       "      <th>binned_entropy(arr, 5)</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_value_number_to_time_series_length(arr)</th>\n",
       "      <th>skewness(arr)</th>\n",
       "      <th>standard_deviation(arr)</th>\n",
       "      <th>sum_of_reoccurring_values(arr)</th>\n",
       "      <th>sum_values(arr)</th>\n",
       "      <th>symmetry_looking(arr, 0.1)</th>\n",
       "      <th>time_reversal_asymmetry_statistic(arr, 2)</th>\n",
       "      <th>value_count(arr, mean(arr))</th>\n",
       "      <th>variance(arr)</th>\n",
       "      <th>int(variance_larger_than_standard_deviation(arr)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.088200e+06</td>\n",
       "      <td>15740.005859</td>\n",
       "      <td>0.016608</td>\n",
       "      <td>-0.001612</td>\n",
       "      <td>-0.274271</td>\n",
       "      <td>4.113637</td>\n",
       "      <td>0.176880</td>\n",
       "      <td>0.031287</td>\n",
       "      <td>0.827967</td>\n",
       "      <td>1.464085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.076787</td>\n",
       "      <td>11.519624</td>\n",
       "      <td>7109.590332</td>\n",
       "      <td>30911.095703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-83.594078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.701736</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.527718e+05</td>\n",
       "      <td>13398.505859</td>\n",
       "      <td>-0.008338</td>\n",
       "      <td>-0.013546</td>\n",
       "      <td>-0.224557</td>\n",
       "      <td>0.455515</td>\n",
       "      <td>0.070873</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.726336</td>\n",
       "      <td>1.130686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.883937</td>\n",
       "      <td>12.635369</td>\n",
       "      <td>3860.458496</td>\n",
       "      <td>28162.408203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-28.822950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.652557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.761332e+07</td>\n",
       "      <td>27890.650391</td>\n",
       "      <td>0.089911</td>\n",
       "      <td>0.005655</td>\n",
       "      <td>-0.433963</td>\n",
       "      <td>8.225249</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292664</td>\n",
       "      <td>0.650519</td>\n",
       "      <td>1.273136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-0.583592</td>\n",
       "      <td>56.365067</td>\n",
       "      <td>3642.331543</td>\n",
       "      <td>272829.843750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9175.507812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3177.020752</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.154056e+05</td>\n",
       "      <td>8028.248047</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>-0.011927</td>\n",
       "      <td>-0.205272</td>\n",
       "      <td>0.499413</td>\n",
       "      <td>0.099070</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.688051</td>\n",
       "      <td>1.147709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1.007509</td>\n",
       "      <td>8.879619</td>\n",
       "      <td>1835.268188</td>\n",
       "      <td>11685.797852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.162048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.847633</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.628311e+04</td>\n",
       "      <td>4691.341309</td>\n",
       "      <td>-0.005037</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.259754</td>\n",
       "      <td>0.384046</td>\n",
       "      <td>0.079796</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.751102</td>\n",
       "      <td>1.031324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.239200</td>\n",
       "      <td>4.341114</td>\n",
       "      <td>830.674927</td>\n",
       "      <td>6118.645020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.979475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.845266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abs_energy(arr)  absolute_sum_of_changes(arr)  \\\n",
       "0     1.088200e+06                  15740.005859   \n",
       "1     9.527718e+05                  13398.505859   \n",
       "2     7.761332e+07                  27890.650391   \n",
       "3     2.154056e+05                   8028.248047   \n",
       "4     5.628311e+04                   4691.341309   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 0)  aggregated_autocorrelation(arr, 1)  \\\n",
       "0                            0.016608                           -0.001612   \n",
       "1                           -0.008338                           -0.013546   \n",
       "2                            0.089911                            0.005655   \n",
       "3                            0.003225                           -0.011927   \n",
       "4                           -0.005037                           -0.009202   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 2)  aggregated_autocorrelation(arr, 3)  \\\n",
       "0                           -0.274271                            4.113637   \n",
       "1                           -0.224557                            0.455515   \n",
       "2                           -0.433963                            8.225249   \n",
       "3                           -0.205272                            0.499413   \n",
       "4                           -0.259754                            0.384046   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 4)  aggregated_autocorrelation(arr, 5)  \\\n",
       "0                            0.176880                            0.031287   \n",
       "1                            0.070873                            0.005023   \n",
       "2                            0.540984                            0.292664   \n",
       "3                            0.099070                            0.009815   \n",
       "4                            0.079796                            0.006367   \n",
       "\n",
       "   approximate_entropy(arr, 4, 0.5)  binned_entropy(arr, 5)  \\\n",
       "0                          0.827967                1.464085   \n",
       "1                          0.726336                1.130686   \n",
       "2                          0.650519                1.273136   \n",
       "3                          0.688051                1.147709   \n",
       "4                          0.751102                1.031324   \n",
       "\n",
       "                         ...                         \\\n",
       "0                        ...                          \n",
       "1                        ...                          \n",
       "2                        ...                          \n",
       "3                        ...                          \n",
       "4                        ...                          \n",
       "\n",
       "   ratio_value_number_to_time_series_length(arr)  skewness(arr)  \\\n",
       "0                                          0.701       0.076787   \n",
       "1                                          0.121       0.883937   \n",
       "2                                          0.987      -0.583592   \n",
       "3                                          0.113       1.007509   \n",
       "4                                          0.115       1.239200   \n",
       "\n",
       "   standard_deviation(arr)  sum_of_reoccurring_values(arr)  sum_values(arr)  \\\n",
       "0                11.519624                     7109.590332     30911.095703   \n",
       "1                12.635369                     3860.458496     28162.408203   \n",
       "2                56.365067                     3642.331543    272829.843750   \n",
       "3                 8.879619                     1835.268188     11685.797852   \n",
       "4                 4.341114                      830.674927      6118.645020   \n",
       "\n",
       "   symmetry_looking(arr, 0.1)  time_reversal_asymmetry_statistic(arr, 2)  \\\n",
       "0                         1.0                                 -83.594078   \n",
       "1                         0.0                                 -28.822950   \n",
       "2                         1.0                               -9175.507812   \n",
       "3                         0.0                                  43.162048   \n",
       "4                         1.0                                  -6.979475   \n",
       "\n",
       "   value_count(arr, mean(arr))  variance(arr)  \\\n",
       "0                          0.0     132.701736   \n",
       "1                          0.0     159.652557   \n",
       "2                          0.0    3177.020752   \n",
       "3                          0.0      78.847633   \n",
       "4                          0.0      18.845266   \n",
       "\n",
       "   int(variance_larger_than_standard_deviation(arr)  \n",
       "0                                               1.0  \n",
       "1                                               1.0  \n",
       "2                                               1.0  \n",
       "3                                               1.0  \n",
       "4                                               1.0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF = pd.DataFrame(data=np.load(\"./features_extracted.npy\"), columns=['abs_energy(arr)',\n",
    "                                                     'absolute_sum_of_changes(arr)',\n",
    "                                                     'aggregated_autocorrelation(arr, 0)',\n",
    "                                                     'aggregated_autocorrelation(arr, 1)',\n",
    "                                                     'aggregated_autocorrelation(arr, 2)',\n",
    "                                                     'aggregated_autocorrelation(arr, 3)',\n",
    "                                                     'aggregated_autocorrelation(arr, 4)',\n",
    "                                                     'aggregated_autocorrelation(arr, 5)',\n",
    "                                                     'approximate_entropy(arr, 4, 0.5)',\n",
    "                                                     'binned_entropy(arr, 5)',\n",
    "                                                     'c3(arr, True)',\n",
    "                                                     'count_above_mean(arr)',\n",
    "                                                     'count_below_mean(arr)',\n",
    "                                                     'cwt_coefficients(arr, Array([1, 2, 3], dtype.s32), 2, 2)',\n",
    "                                                     'energy_ratio_by_chunks(arr, 2, 0)',\n",
    "                                                     'first_location_of_maximum(arr)',\n",
    "                                                     'first_location_of_minimum(arr)',\n",
    "                                                     'has_duplicates(arr)',\n",
    "                                                     'has_duplicate_max(arr)',\n",
    "                                                     'has_duplicate_min(arr)',\n",
    "                                                     'index_mass_quantile(arr, 0.5)',\n",
    "                                                     'kurtosis(arr)',\n",
    "                                                     'large_standard_deviation(arr, 0.4)',\n",
    "                                                     'last_location_of_maximum(arr)',\n",
    "                                                     'last_location_of_minimum(arr)',\n",
    "                                                     'length(arr)()',\n",
    "                                                     'longest_strike_above_mean(arr)',\n",
    "                                                     'longest_strike_below_mean(arr)',\n",
    "                                                     'max_langevin_fixed_point(arr, 7, 2)',\n",
    "                                                     'maximum(arr)',\n",
    "                                                     'mean(arr)',\n",
    "                                                     'mean_absolute_change(arr)',\n",
    "                                                     'mean_change(arr)',\n",
    "                                                     'mean_second_derivative_central(arr)',\n",
    "                                                     'median(arr)',\n",
    "                                                     'minimum(arr)',\n",
    "                                                     'number_crossing_m(arr, 0)',\n",
    "                                                     'number_cwt_peaks(arr,2 )',\n",
    "                                                     'number_peaks(arr, 2)',\n",
    "                                                     'percentage_of_reoccurring_datapoints_to_all_datapoints(arr, False)',\n",
    "                                                     'percentage_of_reoccurring_values_to_all_values(arr, False)',\n",
    "                                                     'quantile(arr, Array([0.6], dtype.f32))',\n",
    "                                                     'ratio_beyond_r_sigma(arr, 0.5)',\n",
    "                                                     'ratio_value_number_to_time_series_length(arr)',\n",
    "                                                     'skewness(arr)',\n",
    "                                                     'standard_deviation(arr)',\n",
    "                                                     'sum_of_reoccurring_values(arr)',\n",
    "                                                     'sum_values(arr)',\n",
    "                                                     'symmetry_looking(arr, 0.1)',\n",
    "                                                     'time_reversal_asymmetry_statistic(arr, 2)',\n",
    "                                                     'value_count(arr, mean(arr))',\n",
    "                                                     'variance(arr)',\n",
    "                                                     'int(variance_larger_than_standard_deviation(arr)'])\n",
    "pandasDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "Next, a clustering based on the features extracted before is going to be carried out giving the user to select the desired number of clusters. \n",
    "\n",
    "Inorder to plot the results, a PCA is applied to use three principal components. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c6bf1f96d742f29435085710adbafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='clusters', min=1), Output()), _…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converts the Pandas dataFrame to a numpy array.\n",
    "X = pandasDF.as_matrix()\n",
    "# Converts NaN to 0.\n",
    "X = np.nan_to_num(X)\n",
    "# Preprocess the Features matirx by scaling it.\n",
    "X = scale(X)\n",
    "# Applies a Principal Component Analysis with a desired number of components equal to three.\n",
    "pca = PCA(n_components=3)\n",
    "# Create a pandas dataframe composed by the principal components.\n",
    "principal_components = pca.fit_transform(X)\n",
    "principal_df = pd.DataFrame(data = principal_components\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "# Plots the results of applying Kmeans with the desired number of clusters.\n",
    "def run_prediction(clusters):\n",
    "    kmeans = KMeans(n_clusters=int(clusters), random_state=0).fit(X)\n",
    "    fig = plt.figure()\n",
    "    Axes3D(fig).scatter(xs=principal_df['principal component 1'], ys = principal_df['principal component 2'], zs=principal_df['principal component 3'], c=kmeans.labels_, s=250)\n",
    "    plt.title(\"Time Series Clustering - K-means( k=\" + str(clusters) + \")\")\n",
    "    plt.show()\n",
    "   \n",
    "interact(run_prediction,clusters=IntSlider(min=1, max=len(X), step=1, continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
