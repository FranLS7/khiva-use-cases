{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Time series clustering using Khiva\n",
    "\n",
    "Clustering time series is a very important method for the analysis of time series since it allows grouping the time series according to its characteristics. Given the clustering result, we can determine for instance which time series forecasting algorithms fit best to each group.\n",
    "\n",
    "Thanks to the features extraction ability of [Khiva](http://khiva-python.readthedocs.io/en/latest/), we can generate a [features](http://khiva-python.readthedocs.io/en/latest/khiva.html#module-khiva.features) matrix to be used as input of any of the already available clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from khiva.features import *\n",
    "from khiva.array import *\n",
    "from khiva.library import * \n",
    "from khiva.dimensionality import *\n",
    "\n",
    "import pandas as df\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we want to clusterize the time series?\n",
    "\n",
    "Actually, the clustering of time series can have a wide variety of uses and applications depending on the context in which it is used.\n",
    "\n",
    "As an example, time series clustering can be used in order to classify them with the target of knowing what time series have got the same characteristics and that way, know which of them could be suitable to concrete forecasting methods (moving average, exponential smoothing, box-jenkins, x-11, etc.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend\n",
    "Prints the backend being used. The CPU, CUDA and OPENCL backends are available in Khiva.  \n",
    "  \n",
    "> This interactive application is being executed in **hub.mybinder** which doesn't provide a GPU and its CPU is quite limited so it is going to take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KHIVABackend.KHIVA_BACKEND_OPENCL\n"
     ]
    }
   ],
   "source": [
    "print(get_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction\n",
    "\n",
    "In the next notebook cell, a total number of 51 features are going to be extracted from 100 time series. The mentioned time series are the result of reducing the original dimensionality to 1000 points. This has been done to speed up the computation. \n",
    "\n",
    "The time series with the dimensionality reduction applied are stored in a file called `time_series_redimensioned.npy` and the code to carry out it is commented in next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../energy/data/data-enerNoc/all-data/csv'\n",
    "file_names = []\n",
    "#array_list = []\n",
    "for filename in os.listdir(path):\n",
    "       if \".csv\" in filename:\n",
    "           file_names.append(filename)\n",
    "#        data = pd.read_csv(path + \"/\" + filename)\n",
    "#        a = visvalingam(Array([range(len(data[\"value\"])), data[\"value\"].as_matrix()]), int(1000))\n",
    "#        arr_tmp = a.get_col(1)\n",
    "#        array_list.append(arr_tmp.to_numpy())\n",
    "#np.save(\"time_series_redimensioned\", np.array(array_list))\n",
    "\n",
    "arr_tmp = Array(np.load('./time_series_redimensioned.npy'))\n",
    "\n",
    "features = np.stack([abs_energy(arr_tmp).to_numpy(),\n",
    "                     absolute_sum_of_changes(arr_tmp).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 0).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 1).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 2).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 3).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 4).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 5).to_numpy(),\n",
    "                     approximate_entropy(arr_tmp, 4, 0.5).to_numpy(),\n",
    "                     binned_entropy(arr_tmp, 5).to_numpy(),\n",
    "                     c3(arr_tmp, True).to_numpy(),\n",
    "                     count_above_mean(arr_tmp).to_numpy(),\n",
    "                     count_below_mean(arr_tmp).to_numpy(),\n",
    "                     cwt_coefficients(arr_tmp, Array([1, 2, 3], dtype.s32), 2, 2).to_numpy(),\n",
    "                     energy_ratio_by_chunks(arr_tmp, 2, 0).to_numpy(),\n",
    "                     first_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                     first_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                     has_duplicates(arr_tmp).to_numpy(),\n",
    "                     has_duplicate_max(arr_tmp).to_numpy(),\n",
    "                     has_duplicate_min(arr_tmp).to_numpy(),\n",
    "                     index_mass_quantile(arr_tmp, 0.5).to_numpy(),\n",
    "                     kurtosis(arr_tmp).to_numpy(),\n",
    "                     large_standard_deviation(arr_tmp, 0.4).to_numpy(),\n",
    "                     last_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                     last_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                     length(arr_tmp).to_numpy(),\n",
    "                     longest_strike_above_mean(arr_tmp).to_numpy(),\n",
    "                     longest_strike_below_mean(arr_tmp).to_numpy(),\n",
    "                     max_langevin_fixed_point(arr_tmp, 7, 2).to_numpy(),\n",
    "                     maximum(arr_tmp).to_numpy(),\n",
    "                     mean(arr_tmp).to_numpy(),\n",
    "                     mean_absolute_change(arr_tmp).to_numpy(),\n",
    "                     mean_change(arr_tmp).to_numpy(),\n",
    "                     mean_second_derivative_central(arr_tmp).to_numpy(),\n",
    "                     median(arr_tmp).to_numpy(),\n",
    "                     minimum(arr_tmp).to_numpy(),\n",
    "                     number_crossing_m(arr_tmp, 0).to_numpy(),\n",
    "                     number_cwt_peaks(arr_tmp, 2).to_numpy(),\n",
    "                     number_peaks(arr_tmp, 2).to_numpy(),\n",
    "                     percentage_of_reoccurring_datapoints_to_all_datapoints(arr_tmp, False).to_numpy(),\n",
    "                     percentage_of_reoccurring_values_to_all_values(arr_tmp, False).to_numpy(),\n",
    "                     quantile(arr_tmp, Array([0.6], dtype.f32)).to_numpy(),\n",
    "                     ratio_beyond_r_sigma(arr_tmp, 0.5).to_numpy(),\n",
    "                     ratio_value_number_to_time_series_length(arr_tmp).to_numpy(),\n",
    "                     skewness(arr_tmp).to_numpy(),\n",
    "                     standard_deviation(arr_tmp).to_numpy(),\n",
    "                     sum_of_reoccurring_values(arr_tmp).to_numpy(),\n",
    "                     sum_values(arr_tmp).to_numpy(),\n",
    "                     symmetry_looking(arr_tmp, 0.1).to_numpy(),\n",
    "                     time_reversal_asymmetry_statistic(arr_tmp, 2).to_numpy(),\n",
    "                     variance(arr_tmp).to_numpy(),\n",
    "                    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can appreciate the 51 features extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_energy(arr)</th>\n",
       "      <th>absolute_sum_of_changes(arr)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 0)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 1)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 2)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 3)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 4)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 5)</th>\n",
       "      <th>approximate_entropy(arr, 4, 0.5)</th>\n",
       "      <th>binned_entropy(arr, 5)</th>\n",
       "      <th>...</th>\n",
       "      <th>quantile(arr, Array([0.6], dtype.f32))</th>\n",
       "      <th>ratio_beyond_r_sigma(arr, 0.5)</th>\n",
       "      <th>ratio_value_number_to_time_series_length(arr)</th>\n",
       "      <th>skewness(arr)</th>\n",
       "      <th>standard_deviation(arr)</th>\n",
       "      <th>sum_of_reoccurring_values(arr)</th>\n",
       "      <th>sum_values(arr)</th>\n",
       "      <th>symmetry_looking(arr, 0.1)</th>\n",
       "      <th>time_reversal_asymmetry_statistic(arr, 2)</th>\n",
       "      <th>variance(arr)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.088199e+06</td>\n",
       "      <td>15740.010742</td>\n",
       "      <td>0.016608</td>\n",
       "      <td>-0.001612</td>\n",
       "      <td>-0.274271</td>\n",
       "      <td>4.113612</td>\n",
       "      <td>0.176879</td>\n",
       "      <td>0.031286</td>\n",
       "      <td>0.827971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>34.275211</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.076788</td>\n",
       "      <td>11.519623</td>\n",
       "      <td>7109.591797</td>\n",
       "      <td>30911.117188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-83.594093</td>\n",
       "      <td>132.701721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.527739e+05</td>\n",
       "      <td>13398.524414</td>\n",
       "      <td>-1.837614</td>\n",
       "      <td>-0.562199</td>\n",
       "      <td>-26.116846</td>\n",
       "      <td>11.289761</td>\n",
       "      <td>6.073333</td>\n",
       "      <td>36.885380</td>\n",
       "      <td>0.726339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.268089</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.883933</td>\n",
       "      <td>12.635367</td>\n",
       "      <td>3860.458496</td>\n",
       "      <td>28162.412109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-28.823009</td>\n",
       "      <td>159.652512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.761327e+07</td>\n",
       "      <td>27890.644531</td>\n",
       "      <td>3.591948</td>\n",
       "      <td>1.296992</td>\n",
       "      <td>-15.567692</td>\n",
       "      <td>289.589630</td>\n",
       "      <td>15.692007</td>\n",
       "      <td>246.239105</td>\n",
       "      <td>0.650522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>288.694885</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-0.583600</td>\n",
       "      <td>56.365086</td>\n",
       "      <td>3642.331543</td>\n",
       "      <td>272830.062500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-9175.567383</td>\n",
       "      <td>3177.022705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.154056e+05</td>\n",
       "      <td>8028.245117</td>\n",
       "      <td>-0.677070</td>\n",
       "      <td>-0.708260</td>\n",
       "      <td>-57.885098</td>\n",
       "      <td>16.710022</td>\n",
       "      <td>7.081564</td>\n",
       "      <td>50.148556</td>\n",
       "      <td>0.688054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.199238</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1.007511</td>\n",
       "      <td>8.879622</td>\n",
       "      <td>1835.268188</td>\n",
       "      <td>11685.800781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.162006</td>\n",
       "      <td>78.847702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.628308e+04</td>\n",
       "      <td>4691.344727</td>\n",
       "      <td>-0.292070</td>\n",
       "      <td>-0.014299</td>\n",
       "      <td>-10.321704</td>\n",
       "      <td>2.325413</td>\n",
       "      <td>1.470899</td>\n",
       "      <td>2.163545</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.970965</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.239205</td>\n",
       "      <td>4.341112</td>\n",
       "      <td>830.674866</td>\n",
       "      <td>6118.645996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.979472</td>\n",
       "      <td>18.845257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abs_energy(arr)  absolute_sum_of_changes(arr)  \\\n",
       "0     1.088199e+06                  15740.010742   \n",
       "1     9.527739e+05                  13398.524414   \n",
       "2     7.761327e+07                  27890.644531   \n",
       "3     2.154056e+05                   8028.245117   \n",
       "4     5.628308e+04                   4691.344727   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 0)  aggregated_autocorrelation(arr, 1)  \\\n",
       "0                            0.016608                           -0.001612   \n",
       "1                           -1.837614                           -0.562199   \n",
       "2                            3.591948                            1.296992   \n",
       "3                           -0.677070                           -0.708260   \n",
       "4                           -0.292070                           -0.014299   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 2)  aggregated_autocorrelation(arr, 3)  \\\n",
       "0                           -0.274271                            4.113612   \n",
       "1                          -26.116846                           11.289761   \n",
       "2                          -15.567692                          289.589630   \n",
       "3                          -57.885098                           16.710022   \n",
       "4                          -10.321704                            2.325413   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 4)  aggregated_autocorrelation(arr, 5)  \\\n",
       "0                            0.176879                            0.031286   \n",
       "1                            6.073333                           36.885380   \n",
       "2                           15.692007                          246.239105   \n",
       "3                            7.081564                           50.148556   \n",
       "4                            1.470899                            2.163545   \n",
       "\n",
       "   approximate_entropy(arr, 4, 0.5)  binned_entropy(arr, 5)      ...        \\\n",
       "0                          0.827971                     NaN      ...         \n",
       "1                          0.726339                     NaN      ...         \n",
       "2                          0.650522                     NaN      ...         \n",
       "3                          0.688054                     NaN      ...         \n",
       "4                          0.751111                     NaN      ...         \n",
       "\n",
       "   quantile(arr, Array([0.6], dtype.f32))  ratio_beyond_r_sigma(arr, 0.5)  \\\n",
       "0                               34.275211                           0.781   \n",
       "1                               31.268089                           0.820   \n",
       "2                              288.694885                           0.532   \n",
       "3                               16.199238                           0.797   \n",
       "4                                2.970965                           0.748   \n",
       "\n",
       "   ratio_value_number_to_time_series_length(arr)  skewness(arr)  \\\n",
       "0                                          0.701       0.076788   \n",
       "1                                          0.121       0.883933   \n",
       "2                                          0.987      -0.583600   \n",
       "3                                          0.113       1.007511   \n",
       "4                                          0.115       1.239205   \n",
       "\n",
       "   standard_deviation(arr)  sum_of_reoccurring_values(arr)  sum_values(arr)  \\\n",
       "0                11.519623                     7109.591797     30911.117188   \n",
       "1                12.635367                     3860.458496     28162.412109   \n",
       "2                56.365086                     3642.331543    272830.062500   \n",
       "3                 8.879622                     1835.268188     11685.800781   \n",
       "4                 4.341112                      830.674866      6118.645996   \n",
       "\n",
       "   symmetry_looking(arr, 0.1)  time_reversal_asymmetry_statistic(arr, 2)  \\\n",
       "0                         1.0                                 -83.594093   \n",
       "1                         0.0                                 -28.823009   \n",
       "2                         1.0                               -9175.567383   \n",
       "3                         0.0                                  43.162006   \n",
       "4                         1.0                                  -6.979472   \n",
       "\n",
       "   variance(arr)  \n",
       "0     132.701721  \n",
       "1     159.652512  \n",
       "2    3177.022705  \n",
       "3      78.847702  \n",
       "4      18.845257  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = features.transpose()\n",
    "pandasDF = pd.DataFrame(data=features, columns=['abs_energy(arr)',\n",
    "                                                     'absolute_sum_of_changes(arr)',\n",
    "                                                     'aggregated_autocorrelation(arr, 0)',\n",
    "                                                     'aggregated_autocorrelation(arr, 1)',\n",
    "                                                     'aggregated_autocorrelation(arr, 2)',\n",
    "                                                     'aggregated_autocorrelation(arr, 3)',\n",
    "                                                     'aggregated_autocorrelation(arr, 4)',\n",
    "                                                     'aggregated_autocorrelation(arr, 5)',\n",
    "                                                     'approximate_entropy(arr, 4, 0.5)',\n",
    "                                                     'binned_entropy(arr, 5)',\n",
    "                                                     'c3(arr, True)',\n",
    "                                                     'count_above_mean(arr)',\n",
    "                                                     'count_below_mean(arr)',\n",
    "                                                     'cwt_coefficients(arr, Array([1, 2, 3], dtype.s32), 2, 2)',\n",
    "                                                     'energy_ratio_by_chunks(arr, 2, 0)',\n",
    "                                                     'first_location_of_maximum(arr)',\n",
    "                                                     'first_location_of_minimum(arr)',\n",
    "                                                     'has_duplicates(arr)',\n",
    "                                                     'has_duplicate_max(arr)',\n",
    "                                                     'has_duplicate_min(arr)',\n",
    "                                                     'index_mass_quantile(arr, 0.5)',\n",
    "                                                     'kurtosis(arr)',\n",
    "                                                     'large_standard_deviation(arr, 0.4)',\n",
    "                                                     'last_location_of_maximum(arr)',\n",
    "                                                     'last_location_of_minimum(arr)',\n",
    "                                                     'length(arr)()',\n",
    "                                                     'longest_strike_above_mean(arr)',\n",
    "                                                     'longest_strike_below_mean(arr)',\n",
    "                                                     'max_langevin_fixed_point(arr, 7, 2)',\n",
    "                                                     'maximum(arr)',\n",
    "                                                     'mean(arr)',\n",
    "                                                     'mean_absolute_change(arr)',\n",
    "                                                     'mean_change(arr)',\n",
    "                                                     'mean_second_derivative_central(arr)',\n",
    "                                                     'median(arr)',\n",
    "                                                     'minimum(arr)',\n",
    "                                                     'number_crossing_m(arr, 0)',\n",
    "                                                     'number_cwt_peaks(arr,2 )',\n",
    "                                                     'number_peaks(arr, 2)',\n",
    "                                                     'percentage_of_reoccurring_datapoints_to_all_datapoints(arr, False)',\n",
    "                                                     'percentage_of_reoccurring_values_to_all_values(arr, False)',\n",
    "                                                     'quantile(arr, Array([0.6], dtype.f32))',\n",
    "                                                     'ratio_beyond_r_sigma(arr, 0.5)',\n",
    "                                                     'ratio_value_number_to_time_series_length(arr)',\n",
    "                                                     'skewness(arr)',\n",
    "                                                     'standard_deviation(arr)',\n",
    "                                                     'sum_of_reoccurring_values(arr)',\n",
    "                                                     'sum_values(arr)',\n",
    "                                                     'symmetry_looking(arr, 0.1)',\n",
    "                                                     'time_reversal_asymmetry_statistic(arr, 2)',\n",
    "                                                     'variance(arr)'])\n",
    "pandasDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "The 51 features calculated in the previous cell are going to be used in order to clusterize the time series. The desired number of clusters can be chosen, as the number of time series to be shown in the plot.\n",
    "\n",
    "In order to plot the results, a PCA is applied to use 3 principal components. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed6b8d1fb58475a932f47e4c86f2bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='Clusters', min=1), IntSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converts the Pandas dataFrame to a numpy array.\n",
    "X = pandasDF.as_matrix()\n",
    "# Converts NaN to 0.\n",
    "X = np.nan_to_num(X)\n",
    "# Preprocess the Features matrix by scaling it.\n",
    "X = scale(X)\n",
    "# Applies a Principal Component Analysis with a desired number of components equal to three.\n",
    "pca = PCA(n_components=3)\n",
    "# Create a Pandas dataFrame composed by the principal components.\n",
    "principal_components = pca.fit_transform(X)\n",
    "principal_df = pd.DataFrame(data = principal_components\n",
    "             , columns = ['PC-1', 'PC-2', 'PC-3'])\n",
    "# Plots the results of applying Kmeans with the desired number of \n",
    "# clusters and the desired number of time series to be shown in the plot\n",
    "def run_prediction(Clusters, TS):\n",
    "    kmeans = KMeans(n_clusters=int(Clusters), random_state=0).fit(X)\n",
    "    fig  = plt.figure()\n",
    "    ax= Axes3D(fig)\n",
    "    ax.scatter(xs=principal_df['PC-1'].as_matrix()[0:TS], \n",
    "               ys = principal_df['PC-2'].as_matrix()[0:TS], \n",
    "               zs=principal_df['PC-3'].as_matrix()[0:TS], \n",
    "               c=kmeans.labels_[0:TS], s=250) \n",
    "\n",
    "    for i, txt in enumerate(file_names[0:TS], 0):\n",
    "        ax.text(principal_df['PC-1'][i],\n",
    "                principal_df['PC-2'][i],principal_df['PC-3'][i],\n",
    "                '%s' % (str(txt)), size=20, zorder=1,  color='k')\n",
    "\n",
    "    ax.set_xlabel('PC-1')\n",
    "    ax.set_ylabel('PC-2')\n",
    "    ax.set_zlabel('PC-3')\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "    \n",
    "interact(run_prediction,TS=IntSlider(min=1, max=len(X), step=1, value = 1,  continuous_update=False), Clusters=IntSlider(min=1, max=len(X), step=1, continuous_update=False));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
